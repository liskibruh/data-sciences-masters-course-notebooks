{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3292b1c1",
   "metadata": {},
   "source": [
    "# Question No. 1:\n",
    "What is the KNN algorithm?\n",
    "\n",
    "## Answer:\n",
    "KNN (K-Nearest Neighbors) is a machine learning algorithm used for classification and regression. It is a non-parametric and instance-based learning algorithm, meaning that it doesn't make any assumptions about the underlying distribution of the data and uses the training instances themselves to make predictions.\n",
    "\n",
    "KNN is a simple and easy-to-implement algorithm, and it can work well on small to medium-sized datasets. However, it can become computationally expensive as the size of the dataset grows since it requires the calculation of the distances between the new data point and all the training instances. It is also sensitive to the choice of the value of K and the distance metric used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9de0d8",
   "metadata": {},
   "source": [
    "# Question No. 2:\n",
    "How do you choose the value of K in KNN?\n",
    "\n",
    "## Answer:\n",
    "Choosing the value of K in KNN is an important hyperparameter tuning step, as it can significantly affect the performance of the algorithm. There are various approaches to selecting the value of K, including:\n",
    "\n",
    "- Rule of thumb: The most common approach is to use the square root of the number of training examples as the value of K. However, this is just a rule of thumb and might not always give the best results.\n",
    "\n",
    "- Cross-validation: Another common approach is to perform k-fold cross-validation to estimate the performance of the algorithm for different values of K. In this method, the dataset is split into k equal-sized folds, and the algorithm is trained and evaluated on each fold, using different values of K. The value of K that results in the best performance (measured by a chosen evaluation metric) across all the folds is then selected.\n",
    "\n",
    "- Grid search: Another approach is to perform a grid search over a range of K values and choose the one that gives the best performance on a held-out validation set. This method can be time-consuming, but it can often result in the best-performing value of K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262d397c",
   "metadata": {},
   "source": [
    "# Question No. 3:\n",
    "What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "## Answer:\n",
    "**KNN classifier** is used when the output variable is categorical or discrete, and the goal is to classify the new input data point into one of the predefined classes. For example, given a dataset of images of cats and dogs with their corresponding labels, a KNN classifier can be trained to classify a new image as either a cat or a dog.\n",
    "\n",
    "**KNN regressor** is used when the output variable is continuous, and the goal is to predict a numerical value. For example, given a dataset of house prices with their corresponding features (e.g., number of bedrooms, area, etc.), a KNN regressor can be trained to predict the price of a new house based on its features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f73dd",
   "metadata": {},
   "source": [
    "# Question No. 4:\n",
    "How do you measure the performance of KNN?\n",
    "\n",
    "## Answer:\n",
    "The performance of the KNN algorithm can be measured using various evaluation metrics, depending on whether it is used for classification or regression tasks. Here are some commonly used metrics:\n",
    "\n",
    "**For classification tasks:**\n",
    "\n",
    "- Accuracy: This is the most common metric for classification tasks, and it measures the percentage of correctly classified instances over the total number of instances.\n",
    "\n",
    "- Precision and Recall: Precision measures the percentage of correctly classified positive instances over the total number of instances classified as positive. Recall, on the other hand, measures the percentage of correctly classified positive instances over the total number of positive instances in the dataset.\n",
    "\n",
    "- F1 Score: This is the harmonic mean of precision and recall and is often used as a more balanced measure that takes both precision and recall into account.\n",
    "\n",
    "- Confusion Matrix: This is a table that shows the number of instances that were correctly or incorrectly classified by the algorithm for each class.\n",
    "\n",
    "**For regression tasks:**\n",
    "\n",
    "- Mean Squared Error (MSE): This measures the average squared difference between the predicted and actual values.\n",
    "\n",
    "- Root Mean Squared Error (RMSE): This is the square root of the MSE and is often used as a more interpretable metric since it is in the same units as the output variable.\n",
    "\n",
    "- R-squared (R2): This measures the proportion of variance in the output variable that can be explained by the algorithm.\n",
    "\n",
    "- Mean Absolute Error (MAE): This measures the average absolute difference between the predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b47a1",
   "metadata": {},
   "source": [
    "# Question No. 5:\n",
    "What is the curse of dimensionality in KNN?\n",
    "\n",
    "## Answer:\n",
    "The curse of dimensionality is a phenomenon that occurs in high-dimensional spaces, where the volume of the space increases exponentially with the number of dimensions. In KNN algorithm, the curse of dimensionality refers to the fact that as the number of features or dimensions in the dataset increases, the performance of the algorithm tends to deteriorate.\n",
    "\n",
    "This is because, as the number of dimensions increases, the distance between data points becomes less informative, and the nearest neighbors of a given point may not be truly representative of its local neighborhood. This is because, in high-dimensional spaces, most of the distance between points is concentrated around the edges or corners of the space, which makes it harder for KNN to accurately identify the nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed85df",
   "metadata": {},
   "source": [
    "# Question No. 6:\n",
    "How do you handle missing values in KNN?\n",
    "\n",
    "## Answer:\n",
    "Here are some common approaches to dealing with missing values in KNN:\n",
    "\n",
    "- Removing instances with missing values: One approach is to simply remove instances with missing values from the dataset. This approach may be suitable if the number of instances with missing values is small compared to the total number of instances. However, this approach can lead to a loss of information if the removed instances contain important information.\n",
    "\n",
    "- Removing features with missing values: Another approach is to remove features with missing values from the dataset. This approach may be suitable if the number of features with missing values is small compared to the total number of features. However, this approach can also lead to a loss of information if the removed features contain important information.\n",
    "\n",
    "- Imputing missing values: Another approach is to impute or fill in the missing values with estimated values. One popular method for imputing missing values in KNN is to use the mean or median value of the non-missing values of the same feature across all instances. Alternatively, other imputation methods such as KNN imputation or regression-based imputation can also be used.\n",
    "\n",
    "- Modifying the distance metric: Another approach is to modify the distance metric used by the KNN algorithm to account for missing values. One such distance metric is the weighted distance metric, where the distance between two instances is calculated based on the features that are not missing for both instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ddfd36",
   "metadata": {},
   "source": [
    "# Question No. 7:\n",
    "Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "## Answer:\n",
    "The KNN classifier and regressor have different performance metrics and are suitable for different types of problems.\n",
    "\n",
    "**KNN Classifier:**\n",
    "The KNN classifier is a supervised learning algorithm used for classification problems. The algorithm predicts the class of an instance based on the class labels of its nearest neighbors. The performance of the KNN classifier can be evaluated using metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "**KNN Regressor:**\n",
    "The KNN regressor is also a supervised learning algorithm used for regression problems. The algorithm predicts the value of an instance based on the values of its nearest neighbors. The performance of the KNN regressor can be evaluated using metrics such as mean squared error (MSE), root mean squared error (RMSE), and R-squared.\n",
    "\n",
    "Comparing the two algorithms, the KNN classifier is better suited for problems where the output variable is categorical or discrete, while the KNN regressor is better suited for problems where the output variable is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9e4f8",
   "metadata": {},
   "source": [
    "# Question No. 8:\n",
    "What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "\n",
    "## Answer:\n",
    "**Strengths of KNN algorithm:**\n",
    "\n",
    "- KNN is a simple algorithm that is easy to implement and understand.\n",
    "- KNN is a non-parametric algorithm, which means it does not make any assumptions about the underlying distribution of the data.\n",
    "- KNN can handle both classification and regression problems.\n",
    "- KNN is a versatile algorithm that can work well with both linear and non-linear data.\n",
    "\n",
    "**Weaknesses of KNN algorithm:**\n",
    "\n",
    "- KNN can be computationally expensive, especially when working with large datasets.\n",
    "- KNN is sensitive to the choice of distance metric and the value of K, which can affect its performance.\n",
    "- KNN can be affected by the curse of dimensionality, which is a problem in high-dimensional data where distances between data points become less informative.\n",
    "- KNN does not handle imbalanced datasets well, as it tends to favor the majority class.\n",
    "\n",
    "**To address the weaknesses of the KNN algorithm**, several techniques can be used:\n",
    "\n",
    "- Dimensionality reduction techniques, such as PCA or t-SNE, can be used to reduce the dimensionality of the data and mitigate the curse of dimensionality.\n",
    "- Feature selection techniques, such as mutual information or chi-square tests, can be used to select the most informative features and reduce the computational cost.\n",
    "- Cross-validation techniques, such as k-fold cross-validation or leave-one-out cross-validation, can be used to tune the value of K and evaluate the performance of the algorithm.\n",
    "- Distance metrics that are more suited for the problem can be used, such as Manhattan distance or Mahalanobis distance.\n",
    "- Sampling techniques, such as oversampling or undersampling, can be used to balance the dataset and improve the performance of KNN on imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4876b1",
   "metadata": {},
   "source": [
    "# Question No. 9:\n",
    "What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "## Answer:\n",
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN algorithm. The main difference between them is how they calculate the distance between two points.\n",
    "\n",
    "**Euclidean distance** is the straight-line distance between two points in a Euclidean space. It is calculated as the square root of the sum of the squares of the differences between the corresponding coordinates of the two points. Mathematically, the Euclidean distance between two points (x1, y1) and (x2, y2) can be expressed as:\n",
    "> d = √((x2 - x1)^2 + (y2 - y1)^2)\n",
    "\n",
    "**Manhattan distance**, also known as taxicab distance or L1 norm, is the distance between two points measured along the axes at right angles. It is calculated as the sum of the absolute differences between the corresponding coordinates of the two points. Mathematically, the Manhattan distance between two points (x1, y1) and (x2, y2) can be expressed as:\n",
    "> d = |x2 - x1| + |y2 - y1|\n",
    "\n",
    "The main difference between Euclidean and Manhattan distance is that Euclidean distance takes into account the magnitude and direction of the differences between the coordinates, while Manhattan distance only takes into account the magnitude of the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a20be63",
   "metadata": {},
   "source": [
    "# Question No. 10:\n",
    "What is the role of feature scaling in KNN?\n",
    "\n",
    "## Answer:\n",
    "Feature scaling is an important preprocessing step in KNN algorithm that helps to improve the performance and accuracy of the algorithm. The reason for this is that KNN is a distance-based algorithm, which means that it relies on the distance between data points to make predictions. If the features have different scales, the distance between the data points will be dominated by the features with the largest scale, leading to biased predictions.\n",
    "\n",
    "By scaling the features, the KNN algorithm can give equal importance to all the features, leading to more accurate and unbiased predictions. Additionally, feature scaling can also help to reduce the computational cost of the algorithm, as it reduces the dimensionality of the feature space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
