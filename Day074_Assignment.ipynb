{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb3808d",
   "metadata": {},
   "source": [
    "# Question No. 1:\n",
    "What is Gradient Boosting Regression?\n",
    "\n",
    "## Answer:\n",
    "Gradient Boosting Regression is a popular machine learning algorithm used for both regression and classification problems. It is an ensemble learning method where multiple weak models are combined to form a stronger model. The algorithm starts by fitting a simple model to the data and then adding subsequent models, each one attempting to correct for the errors of the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380187e",
   "metadata": {},
   "source": [
    "# Question No. 2:\n",
    "Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d27937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.027\n",
      "R-squared: 0.837\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.residuals = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the residuals as the difference between the true values and the mean\n",
    "        self.residuals = y - np.mean(y)\n",
    "        # Fit a tree for each estimator\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, self.residuals)\n",
    "            # Predict the residuals and update them\n",
    "            residuals_pred = tree.predict(X)\n",
    "            self.residuals -= self.learning_rate * residuals_pred\n",
    "            # Add the tree to the list of trees\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predict the residuals for each tree and sum them up\n",
    "        residuals_pred = np.sum(tree.predict(X) for tree in self.trees)\n",
    "        # Return the sum of the residuals and the mean\n",
    "        return np.mean(y) + self.learning_rate * residuals_pred\n",
    "\n",
    "# Generate some random data\n",
    "X = np.random.rand(100, 3)\n",
    "y = np.sum(X, axis=1) + np.random.normal(scale=0.1, size=100)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = X[:80], X[80:]\n",
    "y_train, y_test = y[:80], y[80:]\n",
    "\n",
    "# Train the gradient boosting regressor on the training set\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# Evaluate the model using mean squared error and R-squared\n",
    "mse = np.mean((y_pred - y_test)**2)\n",
    "r2 = 1 - mse / np.var(y_test)\n",
    "\n",
    "print(\"Mean squared error: {:.3f}\".format(mse))\n",
    "print(\"R-squared: {:.3f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5a0f5",
   "metadata": {},
   "source": [
    "# Question No. 3:\n",
    "Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d0462e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "Mean squared error: 6.209\n",
      "R-squared: 0.915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(gbr, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean squared error: {:.3f}\".format(mse))\n",
    "print(\"R-squared: {:.3f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c64bc",
   "metadata": {},
   "source": [
    "# Question No. 4:\n",
    "What is a weak learner in Gradient Boosting?\n",
    "\n",
    "## Answer:\n",
    "In Gradient Boosting, a weak learner is a model that is only slightly better than random guessing. It is a model that has a performance that is only slightly better than a random predictor. Typically, decision trees with shallow depth (1 or 2) are used as weak learners in Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee9a1aa",
   "metadata": {},
   "source": [
    "# Question No. 5:\n",
    "What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "## Answer:\n",
    "The intuition behind Gradient Boosting can be explained as follows:\n",
    "\n",
    "Suppose we have a complex problem that we want to solve using machine learning. We have a large dataset with many features and a target variable that we want to predict. We can train a simple model, such as a linear regression, to predict the target variable, but this model may not be able to capture all the complex patterns in the data.\n",
    "\n",
    "To improve the performance of the model, we can use an ensemble of models. Gradient Boosting is a type of ensemble learning algorithm that combines the predictions of multiple weak learners, such as decision trees, to create a strong learner.\n",
    "\n",
    "The algorithm works as follows: we first train a weak learner on the data, and then calculate the error between the predicted values and the actual values. We then train another weak learner on the residuals (i.e., the difference between the predicted and actual values), and add this learner's predictions to the previous learner's predictions. We continue this process of adding new weak learners until we have a set of models that collectively predict the target variable with high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f25ce",
   "metadata": {},
   "source": [
    "# Question No. 6:\n",
    "How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "## Answer:\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. At each step of the algorithm, a new weak learner is added to the ensemble and its predictions are combined with the predictions of the previous weak learners to improve the overall prediction accuracy.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1. Initialize the ensemble with a constant value, such as the mean or median of the target variable.\n",
    "\n",
    "2. Train a weak learner, such as a decision tree, on the training data. The weak learner is trained to minimize the loss function between the predicted values and the actual values.\n",
    "\n",
    "3. Calculate the residuals between the predicted values and the actual values. These residuals represent the errors made by the current ensemble.\n",
    "\n",
    "4. Train a new weak learner on the residuals. This new learner is trained to predict the residuals, so that it can correct the errors made by the previous learner.\n",
    "\n",
    "5. Combine the predictions of the new learner with the predictions of the previous learners. This is done by adding the new learner's predictions to the predictions of the previous learners, with a weighting factor that determines the contribution of each learner to the final prediction.\n",
    "\n",
    "6. Repeat steps 3 to 5 for a predetermined number of iterations or until a certain level of accuracy is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34febf45",
   "metadata": {},
   "source": [
    "# Question No. 7:\n",
    "What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "\n",
    "## Answer:\n",
    "1. **Define the loss function:** The first step is to define a loss function that measures the difference between the predicted values and the actual values. This loss function is typically a differentiable function, such as the mean squared error or the cross-entropy loss.\n",
    "\n",
    "2. **Initialize the model:** We start with an initial model that predicts the target variable, such as a constant value or the mean of the target variable.\n",
    "\n",
    "3. **Compute the negative gradient of the loss function:** We compute the negative gradient of the loss function with respect to the current model's predictions. This gradient represents the direction in which we should update the model to reduce the loss.\n",
    "\n",
    "4. **Train a new weak learner:** We train a new weak learner, such as a decision tree, on the negative gradient of the loss function. This new learner is trained to predict the negative gradient, so that it can correct the errors made by the previous learner.\n",
    "\n",
    "5. **Update the model:** We update the model by adding the predictions of the new learner to the predictions of the previous model. We use a weighting factor, called the learning rate, to control the contribution of the new learner to the final prediction.\n",
    "\n",
    "6. **Repeat steps 3 to 5:** We repeat steps 3 to 5 for a predetermined number of iterations or until a certain level of accuracy is achieved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
