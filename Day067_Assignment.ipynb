{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15af899",
   "metadata": {},
   "source": [
    "# Question No. 1:\n",
    "A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "## Answer:\n",
    "Let S be the event that an employee is a smoker, and let H be the event that an employee uses the company's health insurance plan.\n",
    "\n",
    "From the problem statement, we know:\n",
    "\n",
    ">P(H) = 0.7 (70% of the employees use the health insurance plan)<br>P(S|H) = 0.4 (40% of the employees who use the plan are smokers)\n",
    "\n",
    "We want to find P(S|H), the probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "Bayes' theorem tells us that:\n",
    "\n",
    ">P(S|H) = P(H|S) * P(S) / P(H)\n",
    "\n",
    "We can solve for P(H|S) using the formula:\n",
    "\n",
    ">P(H|S) = P(S|H) * P(H) / P(S)\n",
    "\n",
    "We know P(S) is not given, but we can use the law of total probability to find it:\n",
    "\n",
    ">P(S) = P(S|H) * P(H) + P(S|H') * P(H')\n",
    "\n",
    "where H' is the event that an employee does not use the health insurance plan.\n",
    "\n",
    "We know P(H') = 1 - P(H) = 0.3, and we can assume that the proportion of smokers among employees who do not use the health insurance plan is negligible, so we can set P(S|H') = 0.\n",
    "\n",
    "Plugging in the values we know, we get:\n",
    "\n",
    ">P(S) = P(S|H) * P(H) + P(S|H') * P(H')<br>P(S) = 0.4 * 0.7 + 0 * 0.3<br>P(S) = 0.28\n",
    "\n",
    "Now we can solve for P(H|S):\n",
    "\n",
    ">P(H|S) = P(S|H) * P(H) / P(S)<br>P(H|S) = 0.4 * 0.7 / 0.28<br>P(H|S) = 1\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 1 or 100%. This means that all employees who use the health insurance plan are smokers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d069f",
   "metadata": {},
   "source": [
    "# Question No. 2:\n",
    "What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "## Answer:\n",
    "**Bernoulli Naive Bayes:**\n",
    "Bernoulli Naive Bayes is used when the features are binary, i.e., they take on only two values, usually 0 and 1. In text classification, the features correspond to the presence or absence of a word in a document. Thus, each feature represents a binary variable that indicates whether a particular word is present or not in the document. Bernoulli Naive Bayes assumes that the features are conditionally independent given the class variable, and estimates the probabilities of each feature being present or absent in the document for each class. The class with the highest probability is then assigned to the document.\n",
    "\n",
    "**Multinomial Naive Bayes:**\n",
    "Multinomial Naive Bayes is used when the features are count data, i.e., they represent the frequency of occurrence of a word in a document. In text classification, the features correspond to the frequency of occurrence of each word in a document. Multinomial Naive Bayes assumes that the features are conditionally independent given the class variable, and estimates the probabilities of each feature taking on a particular count value for each class. The class with the highest probability is then assigned to the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf386001",
   "metadata": {},
   "source": [
    "# Question No. 3:\n",
    "How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "## Answer:\n",
    "Bernoulli Naive Bayes assumes that the features are binary, i.e., they take on only two values, usually 0 and 1. If a feature is missing, it can be treated as a 0 or 1, depending on the approach taken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1d175",
   "metadata": {},
   "source": [
    "# Question No. 4:\n",
    "Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "## Answer:\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification.\n",
    "\n",
    "In Gaussian Naive Bayes, the algorithm assumes that the features are continuous and follows a normal (Gaussian) distribution. For multi-class classification, the algorithm estimates the probability of each class given the features, using Bayes' theorem:\n",
    "\n",
    ">P(class|features) = P(features|class) * P(class) / P(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dde88f",
   "metadata": {},
   "source": [
    "# Question No. 5:\n",
    "**Data preparation:**<br>\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Spambase. This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "**Implementation:**<br>\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "**Results:**<br>\n",
    "Report the following performance metrics for each classifier:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "\n",
    "**Discussion:**<br>\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "**Conclusion:**<br>\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80d68471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3     4     5     6     7     8     9   ...     48  \\\n",
       "0     0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "1     0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.000   \n",
       "2     0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.010   \n",
       "3     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "4     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "...    ...   ...   ...  ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "4596  0.31  0.00  0.62  0.0  0.00  0.31  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4597  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4598  0.30  0.00  0.30  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.102   \n",
       "4599  0.96  0.00  0.00  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4600  0.00  0.00  0.65  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "\n",
       "         49   50     51     52     53     54   55    56  57  \n",
       "0     0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1     0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2     0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3     0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4     0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "...     ...  ...    ...    ...    ...    ...  ...   ...  ..  \n",
       "4596  0.232  0.0  0.000  0.000  0.000  1.142    3    88   0  \n",
       "4597  0.000  0.0  0.353  0.000  0.000  1.555    4    14   0  \n",
       "4598  0.718  0.0  0.000  0.000  0.000  1.404    6   118   0  \n",
       "4599  0.057  0.0  0.000  0.000  0.000  1.147    5    78   0  \n",
       "4600  0.000  0.0  0.125  0.000  0.000  1.250    5    40   0  \n",
       "\n",
       "[4601 rows x 58 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"spambase.data\", header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b36fb655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input features and target variable\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68d57c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the classifiers\n",
    "bernoulli_clf = BernoulliNB()\n",
    "multinomial_clf = MultinomialNB()\n",
    "gaussian_clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6ad1f",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "### Bernoulli NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "645789aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "scoring = ['precision_macro', 'recall_macro','accuracy','f1']\n",
    "scores = cross_validate(bernoulli_clf, X, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c6a765c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of fold 1: 0.8850325379609545\n",
      "Accuracy of fold 2: 0.9173913043478261\n",
      "Accuracy of fold 3: 0.9\n",
      "Accuracy of fold 4: 0.908695652173913\n",
      "Accuracy of fold 5: 0.8913043478260869\n",
      "Accuracy of fold 6: 0.9282608695652174\n",
      "Accuracy of fold 7: 0.9260869565217391\n",
      "Accuracy of fold 8: 0.8913043478260869\n",
      "Accuracy of fold 9: 0.808695652173913\n",
      "Accuracy of fold 10: 0.782608695652174\n",
      "Average Accuracy across all folds: 0.8839380364047911\n",
      "---------------------------------------------------------\n",
      "Precision of fold 1: 0.8975326966098778\n",
      "Precision of fold 2: 0.9176356589147286\n",
      "Precision of fold 3: 0.9137634408602151\n",
      "Precision of fold 4: 0.9114535182331793\n",
      "Precision of fold 5: 0.892552645095018\n",
      "Precision of fold 6: 0.9259502749223045\n",
      "Precision of fold 7: 0.9316890789283427\n",
      "Precision of fold 8: 0.8989952406134321\n",
      "Precision of fold 9: 0.7994194484760523\n",
      "Precision of fold 10: 0.7721867321867322\n",
      "Average Precision across all folds: 0.8861178734839884\n",
      "---------------------------------------------------------\n",
      "Recall of fold 1: 0.8639469849147268\n",
      "Recall of fold 2: 0.9088860779508261\n",
      "Recall of fold 3: 0.8802672147995889\n",
      "Recall of fold 4: 0.8965920117230044\n",
      "Recall of fold 5: 0.8783738291847363\n",
      "Recall of fold 6: 0.9233945226638152\n",
      "Recall of fold 7: 0.9138398780173864\n",
      "Recall of fold 8: 0.8735222479653062\n",
      "Recall of fold 9: 0.8063922057862531\n",
      "Recall of fold 10: 0.774213350759421\n",
      "Average Recall across all folds: 0.8719428323765064\n",
      "---------------------------------------------------------\n",
      "F1 Score of fold 1: 0.8398791540785498\n",
      "F1 Score of fold 2: 0.8926553672316384\n",
      "F1 Score of fold 3: 0.8614457831325301\n",
      "F1 Score of fold 4: 0.8786127167630058\n",
      "F1 Score of fold 5: 0.8554913294797687\n",
      "F1 Score of fold 6: 0.9080779944289694\n",
      "F1 Score of fold 7: 0.9011627906976744\n",
      "F1 Score of fold 8: 0.851190476190476\n",
      "F1 Score of fold 9: 0.7659574468085106\n",
      "F1 Score of fold 10: 0.726775956284153\n",
      "Average F1 Score across all folds: 0.8481249015095276\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#scores for each fold\n",
    "for i in range(0,10):\n",
    "    print(f\"Accuracy of fold {i+1}: {scores['test_accuracy'][i]}\")\n",
    "print(f\"Average Accuracy across all folds: {np.mean(scores['test_accuracy'])}\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Precision of fold {i+1}: {scores['test_precision_macro'][i]}\")\n",
    "print(f\"Average Precision across all folds: {np.mean(scores['test_precision_macro'])}\" )\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Recall of fold {i+1}: {scores['test_recall_macro'][i]}\")   \n",
    "print(f\"Average Recall across all folds: {np.mean(scores['test_recall_macro'])}\" )\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"F1 Score of fold {i+1}: {scores['test_f1'][i]}\") \n",
    "print(f\"Average F1 Score across all folds: {np.mean(scores['test_f1'])}\" )\n",
    "print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d74bef",
   "metadata": {},
   "source": [
    "### Gaussain NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e772bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "scoring = ['precision_macro', 'recall_macro','accuracy','f1']\n",
    "scores = cross_validate(gaussian_clf, X, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91d72c7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of fold 1: 0.8438177874186551\n",
      "Accuracy of fold 2: 0.8630434782608696\n",
      "Accuracy of fold 3: 0.8782608695652174\n",
      "Accuracy of fold 4: 0.8673913043478261\n",
      "Accuracy of fold 5: 0.8847826086956522\n",
      "Accuracy of fold 6: 0.8282608695652174\n",
      "Accuracy of fold 7: 0.8326086956521739\n",
      "Accuracy of fold 8: 0.8673913043478261\n",
      "Accuracy of fold 9: 0.6347826086956522\n",
      "Accuracy of fold 10: 0.717391304347826\n",
      "Average Accuracy across all folds: 0.8217730830896915\n",
      "---------------------------------------------------------\n",
      "Precision of fold 1: 0.8487462292609351\n",
      "Precision of fold 2: 0.8665588162948668\n",
      "Precision of fold 3: 0.8782608695652174\n",
      "Precision of fold 4: 0.8704222154963681\n",
      "Precision of fold 5: 0.8792075070283414\n",
      "Precision of fold 6: 0.8447074142156863\n",
      "Precision of fold 7: 0.8430462568472307\n",
      "Precision of fold 8: 0.8634469696969698\n",
      "Precision of fold 9: 0.7185854544618426\n",
      "Precision of fold 10: 0.7424239475271999\n",
      "Average Precision across all folds: 0.8355405680394657\n",
      "---------------------------------------------------------\n",
      "Recall of fold 1: 0.8642817755720982\n",
      "Recall of fold 2: 0.88289588109732\n",
      "Recall of fold 3: 0.8954858091548739\n",
      "Recall of fold 4: 0.8877700548525713\n",
      "Recall of fold 5: 0.8953147587080932\n",
      "Recall of fold 6: 0.8564823065803283\n",
      "Recall of fold 7: 0.8571555872393513\n",
      "Recall of fold 8: 0.8800075249014832\n",
      "Recall of fold 9: 0.6882512525000495\n",
      "Recall of fold 10: 0.7466484484841284\n",
      "Average Recall across all folds: 0.8454293399090297\n",
      "---------------------------------------------------------\n",
      "F1 Score of fold 1: 0.8293838862559241\n",
      "F1 Score of fold 2: 0.8496420047732697\n",
      "F1 Score of fold 3: 0.8640776699029125\n",
      "F1 Score of fold 4: 0.8537170263788968\n",
      "F1 Score of fold 5: 0.8658227848101266\n",
      "F1 Score of fold 6: 0.8192219679633868\n",
      "F1 Score of fold 7: 0.8205128205128206\n",
      "F1 Score of fold 8: 0.8478802992518704\n",
      "F1 Score of fold 9: 0.6692913385826772\n",
      "F1 Score of fold 10: 0.711111111111111\n",
      "Average F1 Score across all folds: 0.8130660909542995\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#scores for each fold\n",
    "for i in range(0,10):\n",
    "    print(f\"Accuracy of fold {i+1}: {scores['test_accuracy'][i]}\")\n",
    "print(f\"Average Accuracy across all folds: {np.mean(scores['test_accuracy'])}\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Precision of fold {i+1}: {scores['test_precision_macro'][i]}\")\n",
    "print(f\"Average Precision across all folds: {np.mean(scores['test_precision_macro'])}\" )\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Recall of fold {i+1}: {scores['test_recall_macro'][i]}\")   \n",
    "print(f\"Average Recall across all folds: {np.mean(scores['test_recall_macro'])}\" )\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"F1 Score of fold {i+1}: {scores['test_f1'][i]}\") \n",
    "print(f\"Average F1 Score across all folds: {np.mean(scores['test_f1'])}\" )\n",
    "print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc3b868",
   "metadata": {},
   "source": [
    "### Multinomial NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e998147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "scoring = ['precision_macro', 'recall_macro','accuracy','f1']\n",
    "scores = cross_validate(multinomial_clf, X, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b58a4283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of fold 1: 0.7917570498915402\n",
      "Accuracy of fold 2: 0.7934782608695652\n",
      "Accuracy of fold 3: 0.808695652173913\n",
      "Accuracy of fold 4: 0.8347826086956521\n",
      "Accuracy of fold 5: 0.8282608695652174\n",
      "Accuracy of fold 6: 0.7782608695652173\n",
      "Accuracy of fold 7: 0.7782608695652173\n",
      "Accuracy of fold 8: 0.8130434782608695\n",
      "Accuracy of fold 9: 0.6934782608695652\n",
      "Accuracy of fold 10: 0.7434782608695653\n",
      "Average Accuracy across all folds: 0.7863496180326323\n",
      "---------------------------------------------------------\n",
      "Precision of fold 1: 0.7846028180518685\n",
      "Precision of fold 2: 0.7887858083119164\n",
      "Precision of fold 3: 0.8025521852576647\n",
      "Precision of fold 4: 0.8276337066538899\n",
      "Precision of fold 5: 0.8230185909980431\n",
      "Precision of fold 6: 0.7678085051392671\n",
      "Precision of fold 7: 0.7676658476658477\n",
      "Precision of fold 8: 0.8081634339303051\n",
      "Precision of fold 9: 0.6963907384987893\n",
      "Precision of fold 10: 0.7314987714987715\n",
      "Average Precision across all folds: 0.7798120406006364\n",
      "---------------------------------------------------------\n",
      "Recall of fold 1: 0.7744692583402261\n",
      "Recall of fold 2: 0.7731638864732391\n",
      "Recall of fold 3: 0.7933433473001819\n",
      "Recall of fold 4: 0.8249866333986812\n",
      "Recall of fold 5: 0.8137883918493436\n",
      "Recall of fold 6: 0.7667478563931959\n",
      "Recall of fold 7: 0.769658805124854\n",
      "Recall of fold 8: 0.7954216915186438\n",
      "Recall of fold 9: 0.7055882294698905\n",
      "Recall of fold 10: 0.7332224400483178\n",
      "Average Recall across all folds: 0.7750390539916573\n",
      "---------------------------------------------------------\n",
      "F1 Score of fold 1: 0.7241379310344828\n",
      "F1 Score of fold 2: 0.7214076246334311\n",
      "F1 Score of fold 3: 0.7485714285714287\n",
      "F1 Score of fold 4: 0.7877094972067039\n",
      "F1 Score of fold 5: 0.7736389684813754\n",
      "F1 Score of fold 6: 0.7166666666666666\n",
      "F1 Score of fold 7: 0.7213114754098361\n",
      "F1 Score of fold 8: 0.75\n",
      "F1 Score of fold 9: 0.6618705035971223\n",
      "F1 Score of fold 10: 0.6775956284153005\n",
      "Average F1 Score across all folds: 0.7282909724016348\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#scores for each fold\n",
    "for i in range(0,10):\n",
    "    print(f\"Accuracy of fold {i+1}: {scores['test_accuracy'][i]}\")\n",
    "print(f\"Average Accuracy across all folds: {np.mean(scores['test_accuracy'])}\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Precision of fold {i+1}: {scores['test_precision_macro'][i]}\")\n",
    "print(f\"Average Precision across all folds: {np.mean(scores['test_precision_macro'])}\" )\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Recall of fold {i+1}: {scores['test_recall_macro'][i]}\")   \n",
    "print(f\"Average Recall across all folds: {np.mean(scores['test_recall_macro'])}\" )\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"F1 Score of fold {i+1}: {scores['test_f1'][i]}\") \n",
    "print(f\"Average F1 Score across all folds: {np.mean(scores['test_f1'])}\" )\n",
    "print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38125423",
   "metadata": {},
   "source": [
    "## Discussion about the scores:\n",
    "- Comparing the average f1 score across all the folds for the three classifiers we can see that Bernoulli Naive Bayes' Classifier has the best performance on the data.\n",
    "\n",
    "**But during the lecture we discussed that bernoulli is used in cases where the input features are binary. But the input fetures in this given dataset are not binary then how is it performing so good on this data?**\n",
    "- After doing some research ont the internet i found that there could be certain circumstances where Bernoulli Naive Bayes may perform better than Gaussian Naive Bayes or Multinomial Naive Bayes on this type of data. One possible explanation is that the data may have binary-like patterns, such as containing many exact 0's or 1's, or having many features that take on only a few discrete values, which Bernoulli Naive Bayes can exploit. (Which looks like the case here, so that expalins why bernoulli is performing better)\n",
    "\n",
    "So, we will go ahead and train a bernoulli nb classifier for the given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "57290a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5f36e245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c55bbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = bernoulli_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e6e02033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       531\n",
      "           1       0.91      0.80      0.85       390\n",
      "\n",
      "    accuracy                           0.88       921\n",
      "   macro avg       0.89      0.87      0.88       921\n",
      "weighted avg       0.88      0.88      0.88       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluating\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6393d966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_preds, labels=bernoulli_clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=bernoulli_clf.classes_)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdedd5f7",
   "metadata": {},
   "source": [
    "## Summary of confusion matrix\n",
    "- The confusion matrix indicates that the model has correctly predicted the majority of instances for both classes, with a higher accuracy in predicting class 1. However, it has made some incorrect predictions, with a higher number of false negatives for class 0 and false positives for class 1.\n",
    "\n",
    "## Suggestions for future work\n",
    "This is a very basic implementation of training a classifier on the given data. There are a lot of improvements that could be made, some of them are as follows:\n",
    "1. **Pefrom hyperparameter tuning**\n",
    "2. **Implement ensemble techiniques**\n",
    "3. **Try other classifiers like XGBoost and Random Forest etc.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
