{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a18c311-26ae-4f45-96f9-bc8e1ea3fe08",
   "metadata": {},
   "source": [
    "# Question No. 1\n",
    "What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b3719-ac88-424f-8b34-3e65c0cf8b29",
   "metadata": {},
   "source": [
    "Web scraping, also known as web data extraction or web harvesting, is the process of extracting and collecting data from websites using automated tools or software. It involves using a program or script to access the HTML or other structured data on a website and extracting specific information from it.\n",
    "\n",
    "Web scraping is used for various purposes such as gathering market intelligence, research, analytics, and other business-related activities. It is a useful technique for collecting data from websites that do not offer an API or have limited access to their data.\n",
    "\n",
    "**Web scraping can be a valuable tool for data scientists** who need to collect data from various sources, including websites. By extracting data from websites using web scraping, data scientists can gather relevant data to create predictive models, perform statistical analysis, or perform other data-related tasks.\n",
    "\n",
    "Here are three areas where web scraping is commonly used to get data:\n",
    "\n",
    "**E-commerce and price monitoring:** Web scraping can be used to monitor e-commerce websites to collect product data, such as product descriptions, images, and pricing, to track pricing changes and compare prices across multiple sites.\n",
    "\n",
    "**Social media analysis:** Web scraping can be used to collect data from social media platforms, such as Facebook, Twitter, and Instagram, to analyze social media sentiment, track hashtags, and monitor user engagement.\n",
    "\n",
    "**Research and journalism:** Web scraping can be used by researchers and journalists to collect data on various topics, including news articles, public records, and other online sources, to support their research and writing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc384d2-7e6f-422b-89d8-f486190fb958",
   "metadata": {},
   "source": [
    "# Question No. 2:\n",
    "What are the different methods used for Web Scraping?\n",
    "## Answer:\n",
    "There are several methods used for web scraping, each with its own advantages and disadvantages. Here are some of the most commonly used methods:\n",
    "\n",
    "**HTML parsing:** This method involves parsing the HTML source code of a website to extract specific information, such as text or images. HTML parsing can be done using tools such as Beautiful Soup and lxml.\n",
    "\n",
    "**Regular expressions:** This method involves using regular expressions to search for and extract specific patterns of text from a website's HTML code. Regular expressions can be powerful but can also be difficult to create and maintain.\n",
    "\n",
    "**Web scraping tools:** There are many web scraping tools available that can be used to extract data from websites, such as Scrapy, Selenium, and Puppeteer. These tools allow for more advanced web scraping techniques, such as navigating websites, interacting with forms, and handling dynamic content.\n",
    "\n",
    "**APIs:** Some websites offer APIs (Application Programming Interfaces) that can be used to extract data in a structured format. APIs are generally more reliable and easier to use than web scraping, but they may not offer all the data that is available on the website.\n",
    "\n",
    "**Browser extensions:** Some browser extensions, such as Data Miner and Web Scraper, allow users to extract data from websites by interacting with the page directly. These extensions are typically easy to use but may be limited in functionality compared to other web scraping methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7caadf-c924-4aa9-86e3-cd955817a493",
   "metadata": {},
   "source": [
    "# Question No. 3:\n",
    "What is Beautiful Soup? Why is it used?\n",
    "## Answer:\n",
    "Beautiful Soup is a Python library that is used for web scraping purposes. It allows developers to parse HTML and XML documents, and extract useful information from them. The library gets its name from a poem of the same name by Lewis Carroll, which refers to the idea of making sense out of chaos.\n",
    "\n",
    "It is a valuable tool for developers who need to extract data from websites for a variety of applications, including data analysis, machine learning, and research.<br>\n",
    "It allows developers to search for specific tags and attributes, as well as navigate the document tree. With Beautiful Soup, developers can easily extract text, images, links, and other content from a website's HTML code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583bd83-cb31-4f4f-8141-7fd34c621702",
   "metadata": {},
   "source": [
    "# Question No. 4:\n",
    "Why is flask used in this Web Scraping project?\n",
    "## Answer:\n",
    "During the project that was built today, Flask was utilized to construct a basic front-end web application. To accomplish this, a Flask application was first created, which acted as the foundation of the entire project.\n",
    "\n",
    "This web page contained a text box, which allowed the users to input a specific product's name. Additionally, a button was included on this page, which could be clicked to initiate a request for information regarding the product. Upon clicking this button, the user would be directed to a new page, where various product details would be rendered. These details include information such as product pricing, ratings, and reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbc0ff-1892-452d-b665-de63d40743ee",
   "metadata": {},
   "source": [
    "# Question No. 5:\n",
    "Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "## Answer:\n",
    "In today's project we used **AWS CodePipeline** and **AWS Elastic Beanstalk**\n",
    "\n",
    "Here is a very brief summary of the steps that we took during deployement of our web-scrapping web app:\n",
    "1. Created a GitHub repository to store our application's source code.\n",
    "2. Created Elastic Beanstalk Environment\n",
    "3. Created a CodePipeline pipeeline\n",
    "4. Connected our GitHub repository to CodePipeline\n",
    "5. Deployed our web app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
